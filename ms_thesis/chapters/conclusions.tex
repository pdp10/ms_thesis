
\chapter{Conclusions}
\label{conclusions}
In this work, two new individual methods (QMEAN\-rannZ and QMEAN\-eannZ) and three clustering-based methods (QMEAN\-clustZ, QMEAN\-rann\-clustZ and QMEAN\-eann\-clustZ) are developed. In order to assess the quality of models, the former two involve an approach based on neural networks. The distinction is made on the use of a single neural network to predict scores (-rannZ) or a neural network that is adopted only to categorize targets into three quality classes and, then, for each class, an expert neural network to assess scores of target models (-eannZ). Instead, the latter three methods evaluate model scores by taking into account a model ensemble and using a support method such as QMEAN, QMEAN\-rannZ and QMEAN\-eannZ in order to improve the accuracy. A new function, called parabolic fraction modelled, is used to reduce the scores of incomplete generated models. The benefit of this approach is to remove many false positives, avoiding to decrease too much the scores of partial models. Also, with the intention to understand and choose which other new features could be useful, three additional features that consider Gauss integrals, hydrogen bonds and TAP score, are investigated. Finally, the methods categorize targets before predicting model scores and this is shown to improve the accuracy. \\
The implemented individual methods are very fast. In fact, after having trained the neural network, the execution involves only information processing from input to output units. Although the training process requires several hours to complete, mainly due to the training set width and 5-fold cross validation, the running process only depends on the number of neural units that are used and this number is constant. On the other side, features from QMEAN, GIT, HBPLUS and TAP score can be computed once before applying neural network prediction. The algorithm used to cluster target models has a running time of $0(n^{2})$ where $n$ is the number of models proposed for a certain target. To improve the performance, similarity matrices can be computed once previously to the clustering process. \\
The developed clustering methods demonstrate high precision results for all target classes and in particular for the most important high quality models. Data from CASP-4 (40 targets for a total of 4,221 models) and CASP-8 (117 targets comprising 29,064 models) are used to test the proposed programs. The CASP-8 test set allows to compare the developed methods with the programs of other world-wide research teams that have participated to the last CASP experiment, held in December 2008. The individual methods implemented in this thesis, significatively outperform QMEAN between $\approx 3\%$ and $\approx 6\%$ of Pearson correlation. This fact demonstrates that non linear scoring functions that consider a set of features, perform better than linear ones for model quality assessment. The developed clustering-based methods (-clustZ) outperform all other methods participating in CASP-8. In detail, the improvement with respect to ModFOLDclust, which is the best participating method and currently the best MQAP in the world, is of $\approx 1\%$ of global correlation. The best achieved Pearson and Spearman correlations are $\approx 0.9485$ and $\approx 0.9324$ respectively. Also, QMEAN\-clustZ outperforms QMEAN\-clust by $\approx 1.8\%$. Results on the CASP-4 test set are consistent with those in CASP-8.\\
This work opens up many future directions. By selecting additional features besides those from QMEAN, hydrogen bonds are demonstrated to improve the method. This suggests that more investigation could be done in capturing and using these bonds and others, such as disulphide bonds, salt bonds, $\pi$-$\pi$ interactions, $\pi$-cation interactions and hydrophobic interactions, from proteins. The idea underlying Gauss integrals is interesting due to this characterization of proteins from a topological point of view. Using all $29$ descriptors computed by GIT is demonstrated in another work (see \cite{Bau2008}) to underperform with respect to the selection of a subset of them. However, it is possible that the inconsistency achieved by variants of the implemented methods that use Gauss integrals, is motivated by a bad selection of descriptors. Following this consideration, the worse performance achieved using all $29$ descriptors might be due to the great number of features which maybe reduces the importance of features extracted by other methods. A solution could be the definition of a supervised method, i.e. a neural network, that receives the $29$ Gauss invariants computed by GIT from a protein model and returns a single feature. This one would summarize the protein topology by condensing in itself all geometrical descriptors and avoiding to override the importance of other different features.\\
As expected, clustering-based methods significatively outperform individual ones. By weighting clustering scores with QMEAN\-rannZ with respect to QMEAN, performance improve. A interesting question might be the following: is clustering truly or does it tend to be a monotonic function? In other words, using a better support MQAP in order to weight clustering scores, does the performances improve? It would be interesting to use as support MQAP another clustering-based method, i.e. ModFOLDclust or Pcons\_Pcons. If the clustering function was or tended to be monotonic, then clustering scores would have been more accurate than those achieved in this work. Following this motivation, one could estimate the upper bound of the effectiveness reached by the implemented clustering. In fact, using the real GDT\_TS model scores instead of the predictions obtained by a support MQAP, the clustering scores should be the most accurate possible. Assuming the clustering to be a monotonic function, this study would lead to determine an upper bound in this clustering-based method.

\cleardoublepage

